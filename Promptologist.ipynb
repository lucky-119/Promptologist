{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations\n"
      ],
      "metadata": {
        "id": "vqJ54iqhimU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBvuDmucfW6s"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install langid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mrd1PFkoisbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import load_from_disk\n",
        "import langid"
      ],
      "metadata": {
        "id": "Hd55SVD_iVj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Link your drive if you are a colab user\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Aim3bFOz6ksO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as path\n",
        "if path.exists(\"/content\"):\n",
        "  !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "  !sudo apt-get update -qq 2>&1 > /dev/null\n",
        "  !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "  !google-drive-ocamlfuse\n",
        "\n",
        "  !sudo apt-get install -qq w3m # to act as web browser\n",
        "  !xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "  %cd /content\n",
        "  !mkdir drive\n",
        "  %cd drive\n",
        "  !mkdir MyDrive\n",
        "  %cd ..\n",
        "  %cd ..\n",
        "  !google-drive-ocamlfuse -o nonempty /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "1t3lDVccQFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batchSize = 16\n",
        "blockSize = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/drive/MyDrive/train.txt', 'r', encoding='utf-8') as f:\n",
        "    trainData = f.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/valid.txt', 'r', encoding='utf-8') as f:\n",
        "    validData = f.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/test.txt', 'r', encoding='utf-8') as f:\n",
        "    testData = f.read()\n",
        "\n",
        "chars = sorted(list(set(trainData)))\n",
        "vocabSize = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encodeText = lambda s: [stoi[c] for c in s]\n",
        "decodeText = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "trainData = torch.tensor(encodeText(trainData), dtype=torch.long)\n",
        "valData = torch.tensor(encodeText(validData), dtype=torch.long)\n",
        "testData = torch.tensor(encodeText(testData), dtype=torch.long)"
      ],
      "metadata": {
        "id": "J3JoqzggTTvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getBatch(tvt):\n",
        "    if(tvt == 'train'):\n",
        "      data = trainData\n",
        "    elif(tvt == 'test'):\n",
        "      data = testData\n",
        "    else:\n",
        "      data = valData\n",
        "    ix = torch.randint(len(data)-blockSize,(batchSize,))\n",
        "    x = torch.stack([data[i:i+blockSize] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+blockSize+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimateLoss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for tvt in ['train', 'val', 'test']:\n",
        "        losses = torch.zeros(evaluationIterations)\n",
        "        for k in range(evaluationIterations):\n",
        "            X, Y = getBatch(tvt)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[tvt] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "1mct_su9TX78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, headSize):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(numberOfEmbeddings, headSize, bias=False)\n",
        "        self.query = nn.Linear(numberOfEmbeddings, headSize, bias=False)\n",
        "        self.value = nn.Linear(numberOfEmbeddings, headSize, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(blockSize, blockSize)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, headSize):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(headSize) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(numberOfEmbeddings, numberOfEmbeddings)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, numberOfEmbeddings):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(numberOfEmbeddings, 4 * numberOfEmbeddings),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * numberOfEmbeddings, numberOfEmbeddings),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, numberOfEmbeddings, numberOfHeads):\n",
        "        super().__init__()\n",
        "        headSize = numberOfEmbeddings // numberOfHeads\n",
        "        self.sa = MultiHeadAttention(numberOfHeads, headSize)\n",
        "        self.ffwd = FeedFoward(numberOfEmbeddings)\n",
        "        self.ln1 = nn.LayerNorm(numberOfEmbeddings)\n",
        "        self.ln2 = nn.LayerNorm(numberOfEmbeddings)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocabSize, numberOfEmbeddings)\n",
        "        self.position_embedding_table = nn.Embedding(blockSize, numberOfEmbeddings)\n",
        "        self.blocks = nn.Sequential(*[Block(numberOfEmbeddings, numberOfHeads=numberOfHeads) for _ in range(numberOfLayers)])\n",
        "        self.ln_f = nn.LayerNorm(numberOfEmbeddings)\n",
        "        self.lm_head = nn.Linear(numberOfEmbeddings, vocabSize)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, maxNewTokens):\n",
        "\n",
        "        for _ in range(maxNewTokens):\n",
        "            idx_cond = idx[:, -blockSize:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "fxyC4qdmThJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : None,\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return model, optimizer, scheduler, epoch, metric"
      ],
      "metadata": {
        "id": "iD4_4wcxTNf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "lowest_val_loss=1.33\n",
        "epochs = 1000000\n",
        "evalInterval = 100\n",
        "learning_rate = 1e-3\n",
        "evaluationIterations = 200\n",
        "numberOfEmbeddings = 64\n",
        "numberOfHeads = 4\n",
        "numberOfLayers = 4\n",
        "dropout = 0.1\n",
        "epoch_model_path = '/content/drive/MyDrive/Running_Baseline_Checkpoint'\n",
        "best_epoch_model_path = '/content/drive/MyDrive/Best_Baseline_Checkpoint'\n",
        "model = NgramLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "model, optimizer, scheduler, epoch, lowest_val_loss = load_model(epoch_model_path, model, 'low_val_loss', optimizer)\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "for e in range(epochs):\n",
        "\n",
        "    if(e % evalInterval == 0 or e == epochs - 1):\n",
        "        losses = estimateLoss()\n",
        "        print(f\"Step {e}: Train Loss: {losses['train']:.4f}, Val Loss: {losses['val']:.4f}\")\n",
        "        save_model(model, optimizer, None, ['low_val_loss', losses['val']], e, epoch_model_path)\n",
        "        if(losses['val']<lowest_val_loss):\n",
        "          lowest_val_loss=losses['val']\n",
        "          print('Saving Model')\n",
        "          save_model(model, optimizer, None, ['low_val_loss', losses['val']], e, best_epoch_model_path)\n",
        "\n",
        "    xb, yb = getBatch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "bZ_gyT-G1R-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "epoch_model_path = '/content/drive/MyDrive/Running_Baseline_Checkpoint'\n",
        "best_epoch_model_path = '/content/drive/MyDrive/Best_Baseline_Checkpoint'\n",
        "model = NgramLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "model, optimizer, scheduler, epoch, lowest_val_loss = load_model(best_epoch_model_path, model, 'low_val_loss', optimizer)\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "LXdHX_B2_6u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "bertScore = load(\"bertscore\")\n",
        "\n",
        "with open('/content/drive/MyDrive/test_en_1.json', 'r') as json_file:\n",
        "    data = [json.loads(line) for line in json_file]\n",
        "formatted_rows = [f\"Patient: {row['input']}\\nDoctor: {row['output']}\\n\" for row in data]\n",
        "formatted_text = ''.join(formatted_rows)\n",
        "\n",
        "a = formatted_text.split('\\n')[::2]\n",
        "b = formatted_text.split('\\n')[1::2]\n",
        "\n",
        "newText = list(zip(a,b))\n",
        "\n",
        "score=0.0\n",
        "predictions=[]\n",
        "references=[]\n",
        "bleuscores=[]\n",
        "for i in range(len(newText)):\n",
        "  data = torch.tensor(encodeText(newText[i][0]), dtype=torch.long)\n",
        "  context = data.reshape(-1, 1).to(device)\n",
        "  output=decodeText(m.generate(context, maxNewTokens=len(newText[i][1]))[0].tolist())\n",
        "  predictions.append(output)\n",
        "  references.append(newText[i][1])\n",
        "  bscore=sentence_bleu(newText[i][1].split(),output.split())\n",
        "  bleuscores.append(bscore)\n",
        "bleuscoresnp=np.array(bleuscores)"
      ],
      "metadata": {
        "id": "RGmJeXYaR5Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = estimateLoss()\n",
        "print(\"Test Cross Entropy Loss: \",losses['test'])\n",
        "bleuscoresnp=np.array(bleuscores)\n",
        "bscore=bertScore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "print(\"Bert Precision: \", np.average(bscore['precision']))\n",
        "print(\"Bert Recall: \", np.average(bscore['recall']))\n",
        "print(\"Bert F1: \", np.average(bscore['f1']))\n",
        "print(\"Bleu Score: \", np.average(bleuscoresnp))\n",
        "#Bleu Score not valid and a good evaluation metric in our case since it exactly matches each word of both the prediction and references outputs\n",
        "#Bert Score is a good indicator since it takes into consideration a pretrained bert along with cosine similarity to evaluate the outputs."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R__Ma2kEXsqp",
        "outputId": "cfcd14b2-7296-4433-e8b1-0ead66b66bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Cross Entropy Loss:  tensor(1.3273)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bert Precision:  0.7781371203720147\n",
            "Bert Recall:  0.8055340048832814\n",
            "Bert F1:  0.7915615487324149\n",
            "Bleu Score:  4.339856207858473e-157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encodeText(\"So last week I started itching real bad, especially in my legs. I started noticing some bruising (and there were a lot of bruises, BIG bruises) where I scratched. I didnt think I was scratching so hard and Ive never bruised like this before whenever Ive had skin problems where I needed to scratch. Now, I found a lump in my upper thigh and its the size of a quarter. I can only notice it when I touch it and it feels like Im pressing against another bruise. However nothing has appeared on the skin which is leading me to believe its beneath the skin. Any ideas on what it could be?\"), dtype=torch.long)\n",
        "context = data.reshape(-1, 1).to(device)\n",
        "output=decodeText(m.generate(context, maxNewTokens=1000)[0].tolist())"
      ],
      "metadata": {
        "id": "euj6r0Dv_BRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}